# Module imports
import argparse
import os
import sys
import dask.dataframe as dd
import pandas as pd
import git
import dask.array as da

REPO_DIR= git.Repo('.', search_parent_directories=True).working_tree_dir
sys.path.append(f"{REPO_DIR}/libs")

# Custom imports
import DL.utils as utils

def post_process_features(output_dir, slide_type):
    """
	Format extracted histopathological features from bot.train.txt file generated by myslim/bottleneck_predict.py and extract the 1,536 features, tile names. Extract several variables from tile ID.

	Args:
		output_dir (str): path pointing to folder for storing all created files by script

	Returns:
		{output_dir}/features.txt contains the 1,536 features, followed by the sample_submitter_id, tile_ID, slide_submitter_id, Section, Coord_X and Coord_Y and in the rows the tiles
	"""
    
    if slide_type == 'FF':
        features_raw = pd.read_csv(output_dir + "/bot.train.txt", sep="\t", header=None)
        features = features_raw.iloc[:, 2:]
        features.columns = list(range(1536))
        #Add now column variables that define each tile 
        features["tile_ID"] = [utils.get_tile_name(tile_path) for tile_path in features_raw.iloc[:, 0]]
        features["sample_submitter_id"] = features["tile_ID"].str[0:16]
        features["slide_submitter_id"] = features["tile_ID"].str[0:23]
        features["Section"] = features["tile_ID"].str[20:23]
        features["Coord_X"] = [i[-2] for i in features["tile_ID"].str.split("_")]
        features["Coord_Y"] = [i[-1] for i in features["tile_ID"].str.split("_")]
        features.to_csv(output_dir + "/features.txt", sep="\t", header=True)
    
    elif slide_type == 'FFPE':
        features_raw = dd.read_csv(output_dir + "/bot.train.txt", sep = "\t", header=None)
        #features_raw['tile_ID'] = features_raw.iloc[:,0].str[104:180]
        #features_raw['tile_ID'] = features_raw['tile_ID'].str.replace(".jpg'","")
        
        list_tile_ID = [utils.get_tile_name(tile_path) for tile_path in features_raw.iloc[:, 0]]
        #work out the lengths of each partition
        chunks = features_raw.map_partitions(lambda x: len(x)).compute().to_numpy()
        #Build a Dask array with the same partition sizes
        array_tile_ID = da.from_array(list_tile_ID, chunks=tuple(chunks))
        features_raw['tile_ID'] = array_tile_ID
        
        #features_raw['tile_ID'] = features_raw.iloc[:,0].split("/")[-1]
        #features_raw['tile_ID'] = features_raw['tile_ID'].split(".")[0]
        features = features_raw.map_partitions(lambda df: df.drop(columns=[0,1]))
        new_names=list(map(lambda x: str(x), list(range(1536))))
        new_names.append('tile_ID')
        features.columns = new_names
        
        features["sample_submitter_id"] = features["tile_ID"].str[0:16]
        features["slide_submitter_id"] = features["tile_ID"].str[0:23]
        features["Section"] = features["tile_ID"].str[20:23]
        features['Coord_X'] = features['tile_ID'].str.split('_').str[1]
        features['Coord_Y'] = features['tile_ID'].str.split('_').str[-1]
        ## Save features using parquet
        # TODO move function to utils and convert to def instead of lambda
        name_function=lambda x: f"features-{x}.parquet"
        OUTPUT_PATH = f"{output_dir}/features_format_parquet"
        if os.path.exists(OUTPUT_PATH):
            print("Folder exists")
        else:
            os.makedirs(OUTPUT_PATH)
            
        features.to_parquet(path=OUTPUT_PATH, compression='gzip', name_function=name_function)
    
    print("Formatted all features")
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--output_dir", help="Set output folder")
    parser.add_argument("--slide_type", help="Type of tissue slide (FF or FFPE)]")
    args = parser.parse_args()
    post_process_features(output_dir=args.output_dir, slide_type=args.slide_type)
