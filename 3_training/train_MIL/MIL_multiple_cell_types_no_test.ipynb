{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, train_test_split, GroupShuffleSplit\n",
    "import joblib\n",
    "from copy import deepcopy\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "# random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_list = ['CD4 T', 'CD8 T', 'B', 'Macro']\n",
    "feature_extractor = \"PC-CHiP\"\n",
    "output_name = \"CD48_pchip_no_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PC-CHiP\n",
    "features_dir = \"/home/evos/Outputs/CRC/FF/1_extract_histopathological_features/features.txt\"\n",
    "features_histo = pd.read_csv(features_dir, sep=\"\\t\")\n",
    "\n",
    "#UNI\n",
    "# features_dir = \"/home/evos/Outputs/CRC/UNI_features_TCGA/FFPE\"\n",
    "# features_histo_dd = dd.read_parquet(f\"{features_dir}/FFPE_features_batch_*.parquet\")\n",
    "# features_histo = features_histo_dd.compute()\n",
    "# print(len(features_histo)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "features_orion_dir = \"/home/evos/Outputs/CRC/Orion/1_extract_histopathological_features_macenko/features.txt\"\n",
    "features_orion = pd.read_csv(features_orion_dir, sep=\"\\t\")\n",
    "features_orion['slide_submitter_id'] = features_orion['tile_ID'].str.rsplit(\"-registered\", n=1).str[0] + \"-registered\"\n",
    "print(len(features_orion['slide_submitter_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_labels(features_df, rectangle_estimation, cell_type_list):\n",
    "#     scaling = False \n",
    "\n",
    "#     if scaling == True:\n",
    "#         for cell_type in cell_type_list:\n",
    "#             minmax_scaler = MinMaxScaler()\n",
    "#             column = rectangle_estimation[cell_type].values.reshape(-1, 1)\n",
    "#             rectangle_estimation[cell_type] = minmax_scaler.fit_transform(column).flatten()\n",
    "\n",
    "#     def get_base_id(sample_id):\n",
    "#         return sample_id[:-1]  # Removes the last character\n",
    "\n",
    "#     for cell_type in cell_type_list:\n",
    "#         features_df[cell_type] = features_df['sample_submitter_id'].apply(get_base_id).map(rectangle_estimation[cell_type])\n",
    "    \n",
    "#     return features_df\n",
    "\n",
    "# output_path = f\"/home/evos/Outputs/CRC/deconvolution_sc/final_sc_deconv_results/{ann}\"\n",
    "# rectangle_estimation_ =  pd.read_csv(f\"{output_path}/cell_frac_{ann}.csv\", sep=\",\", index_col=0)\n",
    "# rectangle_estimation = rectangle_estimation_.where(rectangle_estimation_ >= 0, 0)\n",
    "# print(rectangle_estimation.columns)\n",
    "\n",
    "# features_hist = add_labels(features_histo, rectangle_estimation, cell_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
      "       ...\n",
      "       'tile_ID', 'sample_submitter_id', 'slide_submitter_id', 'Section',\n",
      "       'Coord_X', 'Coord_Y', 'CD4 T', 'CD8 T', 'B', 'Macro'],\n",
      "      dtype='object', length=1547)\n"
     ]
    }
   ],
   "source": [
    "def add_labels(features_df, rectangle_estimation_dict, cell_type_list):\n",
    "    scaling = False  \n",
    "\n",
    "    if scaling:\n",
    "        for cell_type, data in rectangle_estimation_dict.items():\n",
    "            minmax_scaler = MinMaxScaler()\n",
    "            column = data[cell_type].values.reshape(-1, 1)\n",
    "            rectangle_estimation_dict[cell_type][cell_type] = minmax_scaler.fit_transform(column).flatten()\n",
    "\n",
    "    def get_base_id(sample_id):\n",
    "        return sample_id[:-1]  # Removes the last character\n",
    "\n",
    "    for cell_type in cell_type_list:\n",
    "        if cell_type in rectangle_estimation_dict:\n",
    "            features_df[cell_type] = (\n",
    "                features_df[\"sample_submitter_id\"]\n",
    "                .apply(get_base_id)\n",
    "                .map(rectangle_estimation_dict[cell_type][cell_type])\n",
    "            )\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "\n",
    "# Define paths\n",
    "output_path = \"/home/evos/Outputs/CRC/deconvolution_sc/final_sc_deconv_results\"\n",
    "# Load the different versions\n",
    "rectangle_estimation_ann1 = pd.read_csv(f\"{output_path}/ann1/cell_frac_ann1.csv\", sep=\",\", index_col=0).where(lambda x: x >= 0, 0)\n",
    "rectangle_estimation_ann2 = pd.read_csv(f\"{output_path}/ann2/cell_frac_ann2.csv\", sep=\",\", index_col=0).where(lambda x: x >= 0, 0)\n",
    "\n",
    "# Define which cell type uses which version\n",
    "cell_type_to_ann = {\n",
    "    \"B\": rectangle_estimation_ann1,\n",
    "    \"T\": rectangle_estimation_ann1,\n",
    "    \"EpiT\": rectangle_estimation_ann1,  \n",
    "    \"CD4 T\": rectangle_estimation_ann2,\n",
    "    \"CD8 T\": rectangle_estimation_ann2,  \n",
    "    \"Macro\": rectangle_estimation_ann1,\n",
    "    \"Stromal\": rectangle_estimation_ann1,\n",
    "    \"Endothelial\": rectangle_estimation_ann2,\n",
    "}\n",
    "\n",
    "# Apply function\n",
    "features_hist = add_labels(features_histo, cell_type_to_ann, cell_type_list)\n",
    "print(features_hist.columns)\n",
    "del features_histo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_sampling(features_hist, n_patches_per_slide, min_patches_per_slide=50, sample_all=False):\n",
    "    \"\"\"\n",
    "    Samples a fixed number of patches from each slide in the features_hist DataFrame.\n",
    "    Slides with fewer than min_patches_per_slide patches are excluded.\n",
    "    \n",
    "    Parameters:\n",
    "    - features_hist: pandas DataFrame containing the feature data for each slide\n",
    "    - n_patches_per_slide: Number of patches to sample per slide (ignored if sample_all=True)\n",
    "    - min_patches_per_slide: Minimum number of patches required to include a slide in the sampled data\n",
    "    - sample_all: If True, all patches are sampled, otherwise a fixed number is sampled per slide\n",
    "    \n",
    "    Returns:\n",
    "    - features_hist_sampled: A new DataFrame with sampled patches for each slide\n",
    "    \"\"\"\n",
    "    features_hist_sampled = []\n",
    "\n",
    "    # Group by 'slide_submitter_id' to sample patches for each slide\n",
    "    for slide_id, group in features_hist.groupby('slide_submitter_id'):\n",
    "        # Exclude slides with fewer than the minimum number of patches\n",
    "        if len(group) < min_patches_per_slide:\n",
    "            continue\n",
    "\n",
    "        if sample_all:\n",
    "            # If sampling all patches, just use the entire group\n",
    "            sampled_group = group\n",
    "        else:\n",
    "            # Otherwise, sample a fixed number of patches\n",
    "            n_patches = min(n_patches_per_slide, len(group))\n",
    "            sampled_group = group.sample(n=n_patches, random_state=42)\n",
    "\n",
    "        features_hist_sampled.append(sampled_group)\n",
    "\n",
    "    # Concatenate all the sampled patches back into a single DataFrame\n",
    "    features_hist_sampled = pd.concat(features_hist_sampled, axis=0)\n",
    "\n",
    "    return features_hist_sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "54\n",
      "<built-in function len>\n",
      "221\n",
      "Fold 1:\n",
      "  Training slides: 176\n",
      "  Validation slides: 45\n",
      "Fold 2:\n",
      "  Training slides: 177\n",
      "  Validation slides: 44\n",
      "Fold 3:\n",
      "  Training slides: 177\n",
      "  Validation slides: 44\n",
      "Fold 4:\n",
      "  Training slides: 177\n",
      "  Validation slides: 44\n",
      "Fold 5:\n",
      "  Training slides: 177\n",
      "  Validation slides: 44\n",
      "Test slides: 54\n"
     ]
    }
   ],
   "source": [
    "# Initialize GroupShuffleSplit\n",
    "df_train_val = features_hist.copy()\n",
    "del features_hist\n",
    "\n",
    "#Apply sampling on df_train_val dataset\n",
    "df_train_val = patch_sampling(df_train_val,300,50,False)\n",
    "\n",
    "# Verify the split\n",
    "print(len(df_train_val['slide_submitter_id'].unique()))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=40)  # 5 folds with shuffling\n",
    "folds = []\n",
    "\n",
    "print(len)\n",
    "unique_slide_ids = df_train_val['slide_submitter_id'].unique()\n",
    "print(len(unique_slide_ids))\n",
    "\n",
    "# Generate indices for cross-validation\n",
    "for train_idx, val_idx in kf.split(unique_slide_ids):\n",
    "    train_slide_ids = unique_slide_ids[train_idx]\n",
    "    val_slide_ids = unique_slide_ids[val_idx]\n",
    "    folds.append((train_slide_ids, val_slide_ids))\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(folds):\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    print(f\"  Training slides: {len(train_ids)}\")\n",
    "    print(f\"  Validation slides: {len(val_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MILDataset(Dataset):\n",
    "    def __init__(self, bag_data, label_columns):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            bag_data (dict): A dictionary with slide ids as keys, containing feature arrays and corresponding labels.\n",
    "            label_columns (list): List of column names that represent the cell type labels.\n",
    "        \"\"\"\n",
    "        self.bag_data = bag_data\n",
    "        self.slide_ids = list(bag_data.keys())\n",
    "        self.label_columns = label_columns # Store the label columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bag_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_id = self.slide_ids[idx]\n",
    "        features = torch.tensor(self.bag_data[slide_id]['features'], dtype=torch.float32)\n",
    "        # Get the labels for the multiple cell types\n",
    "        labels = torch.tensor([self.bag_data[slide_id]['label'][col] for col in self.label_columns], dtype=torch.float32)\n",
    "        \n",
    "        return features, labels\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features = [item[0] for item in batch]  # List of feature tensors\n",
    "    labels = torch.stack([item[1] for item in batch])  # Stack the labels into a single tensor (shape: [batch_size, num_cell_types])\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "class MultiHeadMILModel(nn.Module):\n",
    "    def __init__(self, input_size, num_cell_types):\n",
    "        super(MultiHeadMILModel, self).__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "        )\n",
    "        self.heads = nn.ModuleList([nn.Linear(256, 1) for _ in range(num_cell_types)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_features = self.shared_layer(x)\n",
    "        outputs = [torch.sigmoid(head(shared_features)) for head in self.heads]\n",
    "        return torch.cat(outputs, dim=1)  # Shape: [batch_size, num_cell_types]\n",
    "\n",
    "\n",
    "def pearson_correlation_loss(pred, target):\n",
    "    pred = pred - pred.mean(dim=0)\n",
    "    target = target - target.mean(dim=0)\n",
    "    \n",
    "    numerator = torch.sum(pred * target, dim=0)\n",
    "    denominator = torch.sqrt(torch.sum(pred ** 2, dim=0) * torch.sum(target ** 2, dim=0))\n",
    "    \n",
    "    return -torch.mean(numerator / (denominator + 1e-8))  # Add small epsilon for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features_incremental(df_train, df_val, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Standardizes the features in df_train and df_val using an incremental approach.\n",
    "    This avoids fitting the scaler to the entire dataset at once, making it more memory efficient.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_train: pandas DataFrame containing the training data\n",
    "    - df_val: pandas DataFrame containing the validation data\n",
    "    - feature_columns: list of feature column names (if None, assumes all columns except 'label' and identifiers)\n",
    "    \n",
    "    Returns:\n",
    "    - df_train: pandas DataFrame with standardized features for training\n",
    "    - df_val: pandas DataFrame with standardized features for validation\n",
    "    - scaler: StandardScaler object fitted on the training data\n",
    "    \"\"\"\n",
    "    if feature_columns is None:\n",
    "        feature_columns = df_train.columns[:-2]  # Assuming last two columns are 'label' and identifier\n",
    "    \n",
    "    # Initialize variables for incremental mean and variance computation\n",
    "    n_samples = 0\n",
    "    mean_sum = np.zeros(len(feature_columns))\n",
    "    var_sum = np.zeros(len(feature_columns))\n",
    "    \n",
    "    # Incrementally compute the mean and variance for the training data\n",
    "    for slide_id, group in df_train.groupby('slide_submitter_id'):\n",
    "        features = group[feature_columns].values\n",
    "        n_samples += features.shape[0]\n",
    "        mean_sum += features.sum(axis=0)\n",
    "        var_sum += (features ** 2).sum(axis=0)\n",
    "    \n",
    "    # Final mean and variance\n",
    "    mean = mean_sum / n_samples\n",
    "    variance = (var_sum / n_samples) - (mean ** 2)\n",
    "    std_dev = np.sqrt(np.maximum(variance, 1e-8))  # Avoid division by zero\n",
    "    \n",
    "    # Apply the standardization to both the training and validation data\n",
    "    df_train[feature_columns] = (df_train[feature_columns] - mean) / std_dev\n",
    "    df_val[feature_columns] = (df_val[feature_columns] - mean) / std_dev\n",
    "    \n",
    "    # Create a scaler object that contains the mean and standard deviation\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = mean\n",
    "    scaler.scale_ = std_dev\n",
    "    scaler.var_ = variance\n",
    "    \n",
    "    return df_train, df_val, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, labels):\n",
    "        # Assuming predictions is [batch_size, num_labels] and labels is also [batch_size, num_labels]\n",
    "        # Compute MSE for each label independently\n",
    "        loss_per_label = torch.mean((predictions - labels) ** 2, dim=0)  # MSE for each label\n",
    "        total_loss = torch.mean(loss_per_label)  # Average across all labels\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "class MultiLabelPearsonLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelPearsonLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Compute multi-label Pearson correlation loss.\n",
    "        predictions: [batch_size, num_labels]\n",
    "        labels: [batch_size, num_labels]\n",
    "        \"\"\"\n",
    "        eps = 1e-6  # Small value to avoid division by zero\n",
    "\n",
    "        # Compute means\n",
    "        pred_mean = predictions.mean(dim=0, keepdim=True)\n",
    "        label_mean = labels.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Compute numerator (covariance)\n",
    "        covariance = ((predictions - pred_mean) * (labels - label_mean)).sum(dim=0)\n",
    "\n",
    "        # Compute denominator (product of standard deviations)\n",
    "        pred_std = torch.sqrt(((predictions - pred_mean) ** 2).sum(dim=0) + eps)\n",
    "        label_std = torch.sqrt(((labels - label_mean) ** 2).sum(dim=0) + eps)\n",
    "\n",
    "        # Pearson correlation per label\n",
    "        pearson_corr = covariance / (pred_std * label_std + eps)\n",
    "\n",
    "        # Loss: maximize correlation → minimize (1 - correlation)\n",
    "        loss = -pearson_corr.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Training Fold 1\n",
      "Number of tiles in training set for Fold 1: 48567\n",
      "Number of tiles in validation set for Fold 1: 12817\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '1531', '1532', '1533', '1534', '1535', 'slide_submitter_id', 'CD4 T',\n",
      "       'CD8 T', 'B', 'Macro'],\n",
      "      dtype='object', length=1541)\n",
      "Epoch 1/20, Training Loss: -0.2017259650996753, Validation Loss: -0.0119333416223526\n",
      "Epoch 2/20, Training Loss: -0.4865839055606297, Validation Loss: -0.15196972456760705\n",
      "Epoch 3/20, Training Loss: -0.49112526008061, Validation Loss: -0.20200876519083977\n",
      "Epoch 4/20, Training Loss: -0.5563306297574725, Validation Loss: -0.21154868975281715\n",
      "Epoch 5/20, Training Loss: -0.5783770254680088, Validation Loss: -0.19928539171814919\n",
      "Epoch 6/20, Training Loss: -0.5874112801892417, Validation Loss: -0.21351158246397972\n",
      "Epoch 7/20, Training Loss: -0.6579114624432155, Validation Loss: -0.2337368205189705\n",
      "Epoch 8/20, Training Loss: -0.673881539276668, Validation Loss: -0.275733582675457\n",
      "Epoch 9/20, Training Loss: -0.6698358995573861, Validation Loss: -0.29139287769794464\n",
      "Epoch 10/20, Training Loss: -0.7051206656864711, Validation Loss: -0.26714958250522614\n",
      "Epoch 11/20, Training Loss: -0.734175613948277, Validation Loss: -0.2918081432580948\n",
      "Epoch 12/20, Training Loss: -0.7609029838017055, Validation Loss: -0.3359389305114746\n",
      "Epoch 13/20, Training Loss: -0.7406812054770333, Validation Loss: -0.358163520693779\n",
      "Epoch 14/20, Training Loss: -0.7492310915674482, Validation Loss: -0.3858141601085663\n",
      "Epoch 15/20, Training Loss: -0.7929819055965969, Validation Loss: -0.35921184718608856\n",
      "Epoch 16/20, Training Loss: -0.8076597026416233, Validation Loss: -0.33606746792793274\n",
      "Epoch 17/20, Training Loss: -0.8272725343704224, Validation Loss: -0.3694373220205307\n",
      "Epoch 18/20, Training Loss: -0.813645098890577, Validation Loss: -0.3410812169313431\n",
      "Epoch 19/20, Training Loss: -0.8071173088891166, Validation Loss: -0.29909513890743256\n",
      "Epoch 20/20, Training Loss: -0.7740711740085057, Validation Loss: -0.30420123040676117\n",
      "Training Fold 2\n",
      "Number of tiles in training set for Fold 2: 49672\n",
      "Number of tiles in validation set for Fold 2: 11712\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '1531', '1532', '1533', '1534', '1535', 'slide_submitter_id', 'CD4 T',\n",
      "       'CD8 T', 'B', 'Macro'],\n",
      "      dtype='object', length=1541)\n",
      "Epoch 1/20, Training Loss: -0.24629810452461243, Validation Loss: -0.32560765743255615\n",
      "Epoch 2/20, Training Loss: -0.4825030394962856, Validation Loss: -0.3591984659433365\n",
      "Epoch 3/20, Training Loss: -0.5445747971534729, Validation Loss: -0.3741423040628433\n",
      "Epoch 4/20, Training Loss: -0.46354099256651743, Validation Loss: -0.3954477906227112\n",
      "Epoch 5/20, Training Loss: -0.5922881024224418, Validation Loss: -0.40981125831604004\n",
      "Epoch 6/20, Training Loss: -0.5942247254507882, Validation Loss: -0.4158143699169159\n",
      "Epoch 7/20, Training Loss: -0.60905031647001, Validation Loss: -0.40151962637901306\n",
      "Epoch 8/20, Training Loss: -0.6267042330333165, Validation Loss: -0.40244171023368835\n",
      "Epoch 9/20, Training Loss: -0.6534295763288226, Validation Loss: -0.4040432423353195\n",
      "Epoch 10/20, Training Loss: -0.6232007103306907, Validation Loss: -0.4149698317050934\n",
      "Epoch 11/20, Training Loss: -0.718364553792136, Validation Loss: -0.4348362386226654\n",
      "Epoch 12/20, Training Loss: -0.6977483545030866, Validation Loss: -0.44118744134902954\n",
      "Epoch 13/20, Training Loss: -0.7399657879556928, Validation Loss: -0.42875267565250397\n",
      "Epoch 14/20, Training Loss: -0.7725291933332171, Validation Loss: -0.40411755442619324\n",
      "Epoch 15/20, Training Loss: -0.7990813170160566, Validation Loss: -0.4142772853374481\n",
      "Epoch 16/20, Training Loss: -0.8039840885571071, Validation Loss: -0.4289513826370239\n",
      "Epoch 17/20, Training Loss: -0.8064192959240505, Validation Loss: -0.44395872950553894\n",
      "Epoch 18/20, Training Loss: -0.8198321376528058, Validation Loss: -0.44383837282657623\n",
      "Epoch 19/20, Training Loss: -0.8429531029292515, Validation Loss: -0.38955657184123993\n",
      "Epoch 20/20, Training Loss: -0.8650174396378654, Validation Loss: -0.4043201804161072\n",
      "Training Fold 3\n",
      "Number of tiles in training set for Fold 3: 48964\n",
      "Number of tiles in validation set for Fold 3: 12420\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '1531', '1532', '1533', '1534', '1535', 'slide_submitter_id', 'CD4 T',\n",
      "       'CD8 T', 'B', 'Macro'],\n",
      "      dtype='object', length=1541)\n",
      "Epoch 1/20, Training Loss: -0.15170358014958246, Validation Loss: -0.3354921191930771\n",
      "Epoch 2/20, Training Loss: -0.3907139471599034, Validation Loss: -0.42877474427223206\n",
      "Epoch 3/20, Training Loss: -0.4671610040324075, Validation Loss: -0.4855388104915619\n",
      "Epoch 4/20, Training Loss: -0.5203725610460553, Validation Loss: -0.514676958322525\n",
      "Epoch 5/20, Training Loss: -0.5511856121676308, Validation Loss: -0.5398830771446228\n",
      "Epoch 6/20, Training Loss: -0.5947664976119995, Validation Loss: -0.5625991523265839\n",
      "Epoch 7/20, Training Loss: -0.6083277293613979, Validation Loss: -0.5856836438179016\n",
      "Epoch 8/20, Training Loss: -0.6315214123044696, Validation Loss: -0.5882977545261383\n",
      "Epoch 9/20, Training Loss: -0.71613119329725, Validation Loss: -0.5766414701938629\n",
      "Epoch 10/20, Training Loss: -0.6685519303594317, Validation Loss: -0.5699430704116821\n",
      "Epoch 11/20, Training Loss: -0.758699323449816, Validation Loss: -0.5794349610805511\n",
      "Epoch 12/20, Training Loss: -0.7436986480440412, Validation Loss: -0.5945686399936676\n",
      "Epoch 13/20, Training Loss: -0.750318740095411, Validation Loss: -0.546825960278511\n",
      "Epoch 14/20, Training Loss: -0.7884669389043536, Validation Loss: -0.5444918274879456\n",
      "Epoch 15/20, Training Loss: -0.7923546092850822, Validation Loss: -0.504840150475502\n",
      "Epoch 16/20, Training Loss: -0.8235466735703605, Validation Loss: -0.487235426902771\n",
      "Epoch 17/20, Training Loss: -0.8384820733751569, Validation Loss: -0.4973384737968445\n",
      "Epoch 18/20, Training Loss: -0.8313348123005458, Validation Loss: -0.5093825161457062\n",
      "Epoch 19/20, Training Loss: -0.856900657926287, Validation Loss: -0.4672701507806778\n",
      "Epoch 20/20, Training Loss: -0.8616398828370231, Validation Loss: -0.4560420513153076\n",
      "Training Fold 4\n",
      "Number of tiles in training set for Fold 4: 49168\n",
      "Number of tiles in validation set for Fold 4: 12216\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '1531', '1532', '1533', '1534', '1535', 'slide_submitter_id', 'CD4 T',\n",
      "       'CD8 T', 'B', 'Macro'],\n",
      "      dtype='object', length=1541)\n",
      "Epoch 1/20, Training Loss: -0.20616013224103621, Validation Loss: -0.14082521945238113\n",
      "Epoch 2/20, Training Loss: -0.5092357609953199, Validation Loss: -0.1562056690454483\n",
      "Epoch 3/20, Training Loss: -0.5023575510297503, Validation Loss: -0.14323772862553596\n",
      "Epoch 4/20, Training Loss: -0.5526679754257202, Validation Loss: -0.1282770074903965\n",
      "Epoch 5/20, Training Loss: -0.6077505605561393, Validation Loss: -0.11452116817235947\n",
      "Epoch 6/20, Training Loss: -0.6083768606185913, Validation Loss: -0.08203558623790741\n",
      "Epoch 7/20, Training Loss: -0.5777572851095881, Validation Loss: -0.0773116871714592\n",
      "Epoch 8/20, Training Loss: -0.6428482532501221, Validation Loss: -0.10375566780567169\n",
      "Epoch 9/20, Training Loss: -0.6882795180593219, Validation Loss: -0.1104704700410366\n",
      "Epoch 10/20, Training Loss: -0.7517043437276568, Validation Loss: -0.1214950829744339\n",
      "Epoch 11/20, Training Loss: -0.7078931587082999, Validation Loss: -0.11807737499475479\n",
      "Epoch 12/20, Training Loss: -0.7412369676998684, Validation Loss: -0.11075731739401817\n",
      "Epoch 13/20, Training Loss: -0.7406695314816066, Validation Loss: -0.12144161388278008\n",
      "Epoch 14/20, Training Loss: -0.7687891636575971, Validation Loss: -0.10545016080141068\n",
      "Epoch 15/20, Training Loss: -0.7761422225407192, Validation Loss: -0.06574547290802002\n",
      "Epoch 16/20, Training Loss: -0.8285608802522931, Validation Loss: -0.07376301288604736\n",
      "Epoch 17/20, Training Loss: -0.8150391834122794, Validation Loss: -0.0403720885515213\n",
      "Epoch 18/20, Training Loss: -0.7904618893350873, Validation Loss: -0.026449114084243774\n",
      "Epoch 19/20, Training Loss: -0.8523150171552386, Validation Loss: -0.07097999006509781\n",
      "Epoch 20/20, Training Loss: -0.8156815596989223, Validation Loss: -0.06961455196142197\n",
      "Training Fold 5\n",
      "Number of tiles in training set for Fold 5: 49165\n",
      "Number of tiles in validation set for Fold 5: 12219\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '1531', '1532', '1533', '1534', '1535', 'slide_submitter_id', 'CD4 T',\n",
      "       'CD8 T', 'B', 'Macro'],\n",
      "      dtype='object', length=1541)\n",
      "Epoch 1/20, Training Loss: -0.1688217627150672, Validation Loss: -0.35954710841178894\n",
      "Epoch 2/20, Training Loss: -0.43479915601866587, Validation Loss: -0.44874972105026245\n",
      "Epoch 3/20, Training Loss: -0.5372684172221592, Validation Loss: -0.48570840060710907\n",
      "Epoch 4/20, Training Loss: -0.5172922206776482, Validation Loss: -0.49642160534858704\n",
      "Epoch 5/20, Training Loss: -0.5943849171910968, Validation Loss: -0.5016746819019318\n",
      "Epoch 6/20, Training Loss: -0.6416018945830209, Validation Loss: -0.4961290955543518\n",
      "Epoch 7/20, Training Loss: -0.6617465530123029, Validation Loss: -0.503694087266922\n",
      "Epoch 8/20, Training Loss: -0.6574809721537999, Validation Loss: -0.5038716495037079\n",
      "Epoch 9/20, Training Loss: -0.7266459720475333, Validation Loss: -0.505051851272583\n",
      "Epoch 10/20, Training Loss: -0.6863609296934945, Validation Loss: -0.47447875142097473\n",
      "Epoch 11/20, Training Loss: -0.750458402293069, Validation Loss: -0.44766731560230255\n",
      "Epoch 12/20, Training Loss: -0.7560072030339923, Validation Loss: -0.4065691828727722\n",
      "Epoch 13/20, Training Loss: -0.7850605419703892, Validation Loss: -0.47601184248924255\n",
      "Epoch 14/20, Training Loss: -0.7796571680477687, Validation Loss: -0.46587131917476654\n",
      "Epoch 15/20, Training Loss: -0.8344025356428963, Validation Loss: -0.4461243450641632\n",
      "Epoch 16/20, Training Loss: -0.8373984694480896, Validation Loss: -0.48195160925388336\n",
      "Epoch 17/20, Training Loss: -0.8325069461550031, Validation Loss: -0.4771592319011688\n",
      "Epoch 18/20, Training Loss: -0.806540480681828, Validation Loss: -0.48815929889678955\n",
      "Epoch 19/20, Training Loss: -0.8434450966971261, Validation Loss: -0.4068857878446579\n",
      "Epoch 20/20, Training Loss: -0.8866738080978394, Validation Loss: -0.4711402952671051\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "criterion = MultiLabelPearsonLoss()\n",
    "#criterion = pearson_correlation_loss\n",
    "num_epochs = 20\n",
    "n_cell_type = len(cell_type_list)\n",
    "print(n_cell_type)\n",
    "if feature_extractor == \"PC-CHiP\":\n",
    "    input_size=1536\n",
    "elif feature_extractor == \"UNI\":\n",
    "    input_size=1024\n",
    "\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(folds):\n",
    "    model = MultiHeadMILModel(input_size=input_size, num_cell_types=n_cell_type)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    print(f\"Training Fold {fold + 1}\")\n",
    "    \n",
    "    features_ = df_train_val.copy()\n",
    "    # Create training and validation datasets\n",
    "    df_train = features_[features_['slide_submitter_id'].isin(train_ids)]\n",
    "    df_val = features_[features_['slide_submitter_id'].isin(val_ids)]\n",
    "\n",
    "    # Count the number of tiles (patches) in training and validation sets\n",
    "    train_tiles_count = df_train['tile_ID'].nunique()  # or use .shape[0] for number of rows\n",
    "    val_tiles_count = df_val['tile_ID'].nunique()      # assuming 'tile_ID' is the identifier for each patch\n",
    "    \n",
    "    print(f\"Number of tiles in training set for Fold {fold + 1}: {train_tiles_count}\")\n",
    "    print(f\"Number of tiles in validation set for Fold {fold + 1}: {val_tiles_count}\")\n",
    "    \n",
    "    remove_columns = ['Unnamed: 0', 'tile_ID', 'sample_submitter_id', 'Section', 'Coord_X', 'Coord_Y']\n",
    "    # Clean the datasets\n",
    "    df_train_clean = df_train.drop(columns=remove_columns)\n",
    "    print(df_train_clean.columns)\n",
    "    df_val_clean = df_val.drop(columns=remove_columns)\n",
    "\n",
    "    #df_train_clean, df_val_clean, scaler = standardize_features_incremental(df_train_clean, df_val_clean)\n",
    "\n",
    "    #Standardize features (fit on training data and transform both train and validation data)\n",
    "    feature_columns = df_train_clean.columns[:(-1-n_cell_type)]  # Select all feature columns (so no label columns and no 'slide_submitter_id')\n",
    "    scaler = StandardScaler()\n",
    "    df_train_clean[feature_columns] = scaler.fit_transform(df_train_clean[feature_columns])\n",
    "    df_val_clean[feature_columns] = scaler.transform(df_val_clean[feature_columns])\n",
    "    \n",
    "    bag_data_train = {}\n",
    "    for slide_id, group in df_train_clean.groupby('slide_submitter_id'):\n",
    "        features = group.iloc[:, 0:(-1-n_cell_type)].values\n",
    "        labels = group[cell_type_list].iloc[0].to_dict()  # Adjust this part to match your label columns\n",
    "        bag_data_train[slide_id] = {'features': features, 'label': labels}\n",
    "\n",
    "    # For validation data\n",
    "    bag_data_val = {}\n",
    "    for slide_id, group in df_val_clean.groupby('slide_submitter_id'):\n",
    "        features = group.iloc[:, 0:(-1-n_cell_type)].values\n",
    "        labels = group[cell_type_list].iloc[0].to_dict()\n",
    "        bag_data_val[slide_id] = {'features': features, 'label': labels}\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    mil_dataset_train = MILDataset(bag_data_train, cell_type_list)\n",
    "    mil_loader_train = DataLoader(mil_dataset_train, batch_size=28, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    mil_dataset_val = MILDataset(bag_data_val, cell_type_list)\n",
    "    mil_loader_val = DataLoader(mil_dataset_val, batch_size=28, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    #TRAIN THE MODEL\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_features, batch_labels in mil_loader_train:\n",
    "            # Shape of batch_features: [batch_size, num_patches, num_features]\n",
    "            # Shape of batch_labels: [batch_size, num_labels]\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "            batch_predictions = []\n",
    "                \n",
    "            for features in batch_features:  # Iterate through each tensor in the batch\n",
    "            # Shape of 'features': [num_patches, num_features]\n",
    "                patch_predictions = model(features)  #Shape of patch_predictions: [num_patches, num_labels]\n",
    "                aggregated_prediction = patch_predictions.mean(dim=0)  # Aggregate predictions\n",
    "                batch_predictions.append(aggregated_prediction)\n",
    "\n",
    "            batch_predictions = torch.stack(batch_predictions)\n",
    "\n",
    "            #batch_labels = batch_labels.unsqueeze(1)\n",
    "            loss = criterion(batch_predictions, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validate the model after each epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():  # No gradient calculation needed during validation\n",
    "            for val_features, val_labels in mil_loader_val:\n",
    "                batch_predictions = []\n",
    "\n",
    "                for features in val_features:  # Iterate through each tensor in the batch\n",
    "                    patch_predictions = model(features)  # Get predictions for the patches\n",
    "                    aggregated_prediction = patch_predictions.mean(dim=0)  # Aggregate predictions\n",
    "                    batch_predictions.append(aggregated_prediction)\n",
    "\n",
    "                batch_predictions = torch.stack(batch_predictions)\n",
    "        \n",
    "                #val_labels = val_labels.unsqueeze(1)\n",
    "                val_loss = criterion(batch_predictions, val_labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(mil_loader_train)\n",
    "        avg_val_loss = total_val_loss / len(mil_loader_val)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
    "        \n",
    "        #torch.cuda.empty_cache()\n",
    "        #gc.collect()\n",
    "        \n",
    "    torch.save(model.state_dict(), f\"saved_models/model_fold_{fold}.pth\")\n",
    "    joblib.dump(scaler, f\"saved_scalers/scaler_fold_{fold}.pkl\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1542\n",
      "1538\n"
     ]
    }
   ],
   "source": [
    "class MILDatasetTestOrion(Dataset):\n",
    "    def __init__(self, bag_data):\n",
    "        self.bag_data = bag_data\n",
    "        self.slide_ids = list(bag_data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bag_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_id = self.slide_ids[idx]\n",
    "        features = torch.tensor(self.bag_data[slide_id]['features'], dtype=torch.float32)\n",
    "        patch_ids = self.bag_data[slide_id]['patch_ids']  # Patch IDs for the slide\n",
    "        return features, patch_ids  # Return features and patch IDs for prediction\n",
    "\n",
    "remove_columns_test = ['Unnamed: 0', 'sample_submitter_id', 'Section', 'Coord_X', 'Coord_Y']\n",
    "df_test_orion = features_orion.drop(columns=remove_columns_test)\n",
    "\n",
    "print(len(df_test_orion.columns))\n",
    "\n",
    "bag_data_test_orion = {}\n",
    "for slide_id, group in df_test_orion.groupby('slide_submitter_id'):\n",
    "    features = group.iloc[:, 0:(-2)].values  # All feature columns, adjust as needed\n",
    "    patch_ids = group['tile_ID'].tolist()  # Add patch IDs for each slide\n",
    "    bag_data_test_orion[slide_id] = {'features': features, 'patch_ids': patch_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59335/3088891258.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"saved_models/model_fold_{fold}.pth\"))\n"
     ]
    }
   ],
   "source": [
    "#models = torch.load(\"models_all_folds.pth\", weights_only=True)\n",
    "# Initialize dictionaries for models and scalers\n",
    "models = {}\n",
    "scalers = {}\n",
    "\n",
    "# Load models and scalers for each fold\n",
    "for fold in range(5):  # Assuming 5 folds\n",
    "    # Load model\n",
    "    model = MultiHeadMILModel(input_size=input_size, num_cell_types=n_cell_type)# Instantiate the model architecture\n",
    "    model.load_state_dict(torch.load(f\"saved_models/model_fold_{fold}.pth\"))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    models[fold] = model  # Add to the dictionary\n",
    "\n",
    "    # Load scaler\n",
    "    scaler = joblib.load(f\"saved_scalers/scaler_fold_{fold}.pkl\")\n",
    "    scalers[fold] = scaler  # Add to the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Fold 1\n",
      "Evaluating Fold 2\n",
      "Evaluating Fold 3\n",
      "Evaluating Fold 4\n",
      "Evaluating Fold 5\n"
     ]
    }
   ],
   "source": [
    "# Initialize containers for cross-fold results\n",
    "all_final_predictions = {cell_type: [] for cell_type in cell_type_list}  # For storing final predictions per cell type\n",
    "all_patch_probabilities = {cell_type: [] for cell_type in cell_type_list}  # For storing patch probabilities per cell type\n",
    "\n",
    "all_patch_IDs_orion = []\n",
    "# Populate the list with patch IDs from all slides\n",
    "for slide_id, group in bag_data_test_orion.items():\n",
    "    all_patch_IDs_orion.extend(group['patch_ids'])\n",
    "\n",
    "for fold in range(5):  # Assuming 5 folds\n",
    "    print(f\"Evaluating Fold {fold + 1}\")\n",
    "    \n",
    "    # Retrieve the model and scaler for the current fold\n",
    "    model = models[fold]\n",
    "    scaler = scalers[fold]\n",
    "    \n",
    "    # Normalize the test data using the current fold's scaler\n",
    "    normalized_bag_data_orion = deepcopy(bag_data_test_orion)\n",
    "    for slide_id in normalized_bag_data_orion:\n",
    "        # Create a DataFrame with the same column names used during training\n",
    "        features_df = pd.DataFrame(\n",
    "            normalized_bag_data_orion[slide_id]['features'], \n",
    "            columns=scaler.feature_names_in_  # Use feature names from the scaler\n",
    "        )\n",
    "        # Normalize using the scaler\n",
    "        normalized_bag_data_orion[slide_id]['features'] = scaler.transform(features_df)\n",
    "    \n",
    "    # Create the test dataset and DataLoader\n",
    "    mil_dataset_orion = MILDatasetTestOrion(normalized_bag_data_orion)\n",
    "    mil_loader_orion = DataLoader(mil_dataset_orion, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Collect predictions for the current fold\n",
    "    fold_patch_probabilities = {cell_type: [] for cell_type in cell_type_list}  # For storing patch-level predictions per cell type\n",
    "    fold_final_predictions = {cell_type: [] for cell_type in cell_type_list}  # For storing final aggregated predictions per cell type\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for features, patch_ids in mil_loader_orion:\n",
    "            # Get patch probabilities (since each bag has size 1)\n",
    "            patch_predictions = model(features.squeeze(0))  # Remove extra dimension\n",
    "            \n",
    "            # Collect patch probabilities for each cell type\n",
    "            for idx, cell_type in enumerate(cell_type_list):\n",
    "                fold_patch_probabilities[cell_type].append(patch_predictions[:,idx].numpy())  # Store the prediction for the specific cell type\n",
    "\n",
    "            # Aggregate to get the final prediction (mean of patch probabilities) for each cell type\n",
    "            for cell_type in cell_type_list:\n",
    "                final_prediction = patch_predictions[idx].mean().item()\n",
    "                fold_final_predictions[cell_type].append(final_prediction)\n",
    "    \n",
    "    # Store fold results for each cell type\n",
    "    for cell_type in cell_type_list:\n",
    "        all_patch_probabilities[cell_type].append(fold_patch_probabilities[cell_type])\n",
    "        all_final_predictions[cell_type].append(fold_final_predictions[cell_type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192690\n",
      "Results for CD4 T cell type:\n",
      "Average final predictions: [0.292956   0.48908759 0.15840869 0.17797678 0.10452881 0.20074659\n",
      " 0.21284911 0.43890408 0.29662629 0.29043722 0.17358346 0.38925174\n",
      " 0.07060091 0.06831405 0.3981499  0.18360193 0.411857   0.36778786\n",
      " 0.28382655 0.10866483 0.35030594 0.24087923 0.185508   0.4065733\n",
      " 0.10769923 0.17673828 0.27067699 0.01551191 0.05228679 0.25357209\n",
      " 0.23071566 0.45553837 0.30896691 0.41917271 0.50811113 0.49043934\n",
      " 0.23614226 0.51505275 0.40114164 0.37701266 0.21340441]\n",
      "Results for CD8 T cell type:\n",
      "Average final predictions: [0.292956   0.48908759 0.15840869 0.17797678 0.10452881 0.20074659\n",
      " 0.21284911 0.43890408 0.29662629 0.29043722 0.17358346 0.38925174\n",
      " 0.07060091 0.06831405 0.3981499  0.18360193 0.411857   0.36778786\n",
      " 0.28382655 0.10866483 0.35030594 0.24087923 0.185508   0.4065733\n",
      " 0.10769923 0.17673828 0.27067699 0.01551191 0.05228679 0.25357209\n",
      " 0.23071566 0.45553837 0.30896691 0.41917271 0.50811113 0.49043934\n",
      " 0.23614226 0.51505275 0.40114164 0.37701266 0.21340441]\n",
      "Results for B cell type:\n",
      "Average final predictions: [0.292956   0.48908759 0.15840869 0.17797678 0.10452881 0.20074659\n",
      " 0.21284911 0.43890408 0.29662629 0.29043722 0.17358346 0.38925174\n",
      " 0.07060091 0.06831405 0.3981499  0.18360193 0.411857   0.36778786\n",
      " 0.28382655 0.10866483 0.35030594 0.24087923 0.185508   0.4065733\n",
      " 0.10769923 0.17673828 0.27067699 0.01551191 0.05228679 0.25357209\n",
      " 0.23071566 0.45553837 0.30896691 0.41917271 0.50811113 0.49043934\n",
      " 0.23614226 0.51505275 0.40114164 0.37701266 0.21340441]\n",
      "Results for Macro cell type:\n",
      "Average final predictions: [0.292956   0.48908759 0.15840869 0.17797678 0.10452881 0.20074659\n",
      " 0.21284911 0.43890408 0.29662629 0.29043722 0.17358346 0.38925174\n",
      " 0.07060091 0.06831405 0.3981499  0.18360193 0.411857   0.36778786\n",
      " 0.28382655 0.10866483 0.35030594 0.24087923 0.185508   0.4065733\n",
      " 0.10769923 0.17673828 0.27067699 0.01551191 0.05228679 0.25357209\n",
      " 0.23071566 0.45553837 0.30896691 0.41917271 0.50811113 0.49043934\n",
      " 0.23614226 0.51505275 0.40114164 0.37701266 0.21340441]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_patch_IDs_orion))\n",
    "average_patch_probabilities_orion = {}\n",
    "\n",
    "for cell_type in cell_type_list:\n",
    "    print(f\"Results for {cell_type} cell type:\")\n",
    "\n",
    "    # Calculate average final predictions for this cell type\n",
    "    average_final_predictions = np.mean(all_final_predictions[cell_type], axis=0)\n",
    "\n",
    "    # Concatenate patch probabilities for this cell type\n",
    "    concatenated_patch_probabilities = []\n",
    "    for fold in range(5):\n",
    "        concatenated_patches = np.concatenate(all_patch_probabilities[cell_type][fold], axis=0)\n",
    "        concatenated_patch_probabilities.append(concatenated_patches)\n",
    "    \n",
    "    # Average the concatenated patch probabilities for the current cell type\n",
    "    average_patch_probabilities = np.mean(concatenated_patch_probabilities, axis=0)\n",
    "    # Add the result to the dictionary\n",
    "    average_patch_probabilities_orion[cell_type] = average_patch_probabilities\n",
    "\n",
    "    # Print average final predictions and true labels\n",
    "    print(\"Average final predictions:\", average_final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the final DataFrame list\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through the patch IDs (assumed to be ordered in the same way across cell types)\n",
    "for idx, patch_ids in enumerate(all_patch_IDs_orion):  # Iterate through the list of patch IDs\n",
    "    row_data = {'tile_ID': patch_ids}  # Start the row with the patch IDs\n",
    "    # Add the corresponding probabilities for each cell type\n",
    "    for cell_type in cell_type_list:\n",
    "        row_data[cell_type] = average_patch_probabilities_orion[cell_type][idx]\n",
    "\n",
    "    df = pd.DataFrame([row_data])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one big DataFrame and add columns\n",
    "final_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "final_dataframe['tile_ID'] = final_dataframe['tile_ID'].astype(str)\n",
    "split_columns = final_dataframe['tile_ID'].str.rsplit(\"_\", n=2, expand=True)\n",
    "final_dataframe['slide_id'] = split_columns[0]\n",
    "final_dataframe['Coord_X'] = split_columns[1]\n",
    "final_dataframe['Coord_Y'] = split_columns[2]\n",
    "\n",
    "# Set the output directory\n",
    "full_output_dir = \"/home/evos/Outputs/CRC/Orion/MIL_results/MIL_multiple_cell_types\"\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_dataframe.to_csv(f\"{full_output_dir}/{output_name}.csv\", sep=\"\\t\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
