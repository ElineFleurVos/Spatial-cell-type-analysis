{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, train_test_split, GroupShuffleSplit\n",
    "import joblib\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "# random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation_loss(pred, target):\n",
    "    pred = pred - pred.mean(dim=0)\n",
    "    target = target - target.mean(dim=0)\n",
    "    \n",
    "    numerator = torch.sum(pred * target, dim=0)\n",
    "    denominator = torch.sqrt(torch.sum(pred ** 2, dim=0) * torch.sum(target ** 2, dim=0))\n",
    "    \n",
    "    return -torch.mean(numerator / (denominator + 1e-8))  # Add small epsilon for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables\n",
    "ann = 'ann1'\n",
    "scaling = True\n",
    "cell_type = 'Stromal' #'EpiT, T', 'Endothelial'\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = pearson_correlation_loss\n",
    "cell_type_name = 'stromal_cells' #tumor_purity, endothelial_cells, T_cells\n",
    "#full_output_dir = \"/home/evos/Outputs/CRC/Orion/MIL_results/MIL_old_deconv\"\n",
    "full_output_dir = \"/home/evos/Outputs/CRC/Orion/MIL_results/MIL_extend_cell_types\"\n",
    "output_name = f\"pcchip_{cell_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220573\n"
     ]
    }
   ],
   "source": [
    "#FF\n",
    "features_dir = \"/home/evos/Outputs/CRC/FF/1_extract_histopathological_features/features.txt\"\n",
    "features_histo = pd.read_csv(features_dir, sep=\"\\t\")\n",
    "\n",
    "#FFPE\n",
    "# features_dir = \"/home/evos/Outputs/CRC/FFPE/1_extract_histopathological_features/features_format_parquet\"\n",
    "# features_histo_dd = dd.read_parquet(f\"{features_dir}/features-*.parquet\")\n",
    "# features_histo = features_histo_dd.compute()\n",
    "# del features_histo_dd\n",
    "\n",
    "print(len(features_histo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "features_orion_dir = \"/home/evos/Outputs/CRC/Orion/1_extract_histopathological_features_macenko/features.txt\"\n",
    "features_orion = pd.read_csv(features_orion_dir, sep=\"\\t\")\n",
    "features_orion['slide_submitter_id'] = features_orion['tile_ID'].str.rsplit(\"-registered\", n=1).str[0] + \"-registered\"\n",
    "print(len(features_orion['slide_submitter_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add labels first deconv\n",
    "# output_path = f\"/home/evos/Outputs/CRC/deconvolution_sc/averaged_deconv_first_gen.csv\"\n",
    "# rectangle_estimation_first_gen =  pd.read_csv(output_path, sep=\",\", index_col=0)\n",
    "# print(rectangle_estimation_first_gen.columns)\n",
    "\n",
    "# # Filter rows with valid (non-NaN) values in the cell_type column for normalization\n",
    "# filtered_df = rectangle_estimation_first_gen[[cell_type]].dropna()\n",
    "# minmax_scaler = MinMaxScaler()\n",
    "# filtered_df[cell_type] = minmax_scaler.fit_transform(filtered_df[cell_type].values.reshape(-1, 1)) #normalize the column\n",
    "\n",
    "# filtered_df = filtered_df.drop_duplicates(keep='first')\n",
    "# print(len(filtered_df))\n",
    "\n",
    "# def get_base_id(sample_id):\n",
    "#     return sample_id[:-1]  # Removes the last character\n",
    "\n",
    "# features_histo['label'] = features_histo['sample_submitter_id'].apply(get_base_id).map(filtered_df[cell_type])\n",
    "# features_histo = features_histo.dropna(subset=['label'])\n",
    "\n",
    "# print(features_histo.head())\n",
    "# print(len(features_histo))\n",
    "\n",
    "# # features_histo = merged_features_histo.copy()\n",
    "# # del merged_features_histo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['B', 'EpiT', 'Macro', 'Mast', 'Plasma', 'Stromal', 'T', 'Unknown'], dtype='object')\n",
      "220573\n"
     ]
    }
   ],
   "source": [
    "#Add labels second deconv\n",
    "\n",
    "output_path = f\"/home/evos/Outputs/CRC/deconvolution_sc/final_sc_deconv_results/{ann}\"\n",
    "rectangle_estimation_ =  pd.read_csv(f\"{output_path}/cell_frac_{ann}.csv\", sep=\",\", index_col=0)\n",
    "rectangle_estimation = rectangle_estimation_.where(rectangle_estimation_ >= 0, 0)\n",
    "rectangle_estimation.columns = rectangle_estimation.columns.str.replace(\" \", \"_\", regex=False)\n",
    "print(rectangle_estimation.columns)\n",
    "\n",
    "if scaling == True:\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    cell_type_column = rectangle_estimation[cell_type].values.reshape(-1, 1)\n",
    "    scaled_column = minmax_scaler.fit_transform(cell_type_column)\n",
    "    rectangle_estimation_normalized = rectangle_estimation.copy() \n",
    "    rectangle_estimation_normalized[cell_type] = scaled_column.flatten()\n",
    "\n",
    "    def get_base_id(sample_id):\n",
    "        return sample_id[:-1]  # Removes the last character\n",
    "    # Create a new column in df1 by mapping the modified 'sample_submitter_id'\n",
    "    features_histo['label'] = features_histo['sample_submitter_id'].apply(get_base_id).map(rectangle_estimation_normalized[cell_type])\n",
    "\n",
    "if scaling == False:\n",
    "    def get_base_id(sample_id):\n",
    "        return sample_id[:-1]  # Removes the last character\n",
    "    # Create a new column in df1 by mapping the modified 'sample_submitter_id'\n",
    "    features_histo['label'] = features_histo['sample_submitter_id'].apply(get_base_id).map(rectangle_estimation[cell_type])\n",
    "\n",
    "print(len(features_histo))\n",
    "#del rectangle_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_sampling(features_hist, n_patches_per_slide, min_patches_per_slide=100, sample_all=False):\n",
    "    \"\"\"\n",
    "    Samples a fixed number of patches from each slide in the features_hist DataFrame.\n",
    "    Slides with fewer than min_patches_per_slide patches are excluded.\n",
    "    \n",
    "    Parameters:\n",
    "    - features_hist: pandas DataFrame containing the feature data for each slide\n",
    "    - n_patches_per_slide: Number of patches to sample per slide (ignored if sample_all=True)\n",
    "    - min_patches_per_slide: Minimum number of patches required to include a slide in the sampled data\n",
    "    - sample_all: If True, all patches are sampled, otherwise a fixed number is sampled per slide\n",
    "    \n",
    "    Returns:\n",
    "    - features_hist_sampled: A new DataFrame with sampled patches for each slide\n",
    "    \"\"\"\n",
    "    features_hist_sampled = []\n",
    "\n",
    "    # Group by 'slide_submitter_id' to sample patches for each slide\n",
    "    for slide_id, group in features_hist.groupby('slide_submitter_id'):\n",
    "        # Exclude slides with fewer than the minimum number of patches\n",
    "        if len(group) < min_patches_per_slide:\n",
    "            continue\n",
    "\n",
    "        if sample_all:\n",
    "            # If sampling all patches, just use the entire group\n",
    "            sampled_group = group\n",
    "        else:\n",
    "            # Otherwise, sample a fixed number of patches\n",
    "            n_patches = min(n_patches_per_slide, len(group))\n",
    "            sampled_group = group.sample(n=n_patches, random_state=42)\n",
    "\n",
    "        features_hist_sampled.append(sampled_group)\n",
    "\n",
    "    # Concatenate all the sampled patches back into a single DataFrame\n",
    "    features_hist_sampled = pd.concat(features_hist_sampled, axis=0)\n",
    "\n",
    "    return features_hist_sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220573\n",
      "275\n",
      "<built-in function len>\n",
      "275\n",
      "Fold 1:\n",
      "  Training slides: 220\n",
      "  Validation slides: 55\n",
      "Fold 2:\n",
      "  Training slides: 220\n",
      "  Validation slides: 55\n",
      "Fold 3:\n",
      "  Training slides: 220\n",
      "  Validation slides: 55\n",
      "Fold 4:\n",
      "  Training slides: 220\n",
      "  Validation slides: 55\n",
      "Fold 5:\n",
      "  Training slides: 220\n",
      "  Validation slides: 55\n",
      "Train/val patches: 220548\n"
     ]
    }
   ],
   "source": [
    "# Initialize GroupShuffleSplit\n",
    "n_randomstate = 41 #40,41,42,43,44\n",
    "print(len(features_histo))\n",
    "\n",
    "df_train_val = features_histo.copy()\n",
    "del features_histo\n",
    "\n",
    "#Apply sampling on df_train_val dataset\n",
    "df_train_val = patch_sampling(df_train_val,500,50,sample_all=True)\n",
    "\n",
    "# Verify the split\n",
    "print(len(df_train_val['slide_submitter_id'].unique()))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=n_randomstate) # 5 folds with shuffling #40,41,42,43,54\n",
    "folds = []\n",
    "\n",
    "print(len)\n",
    "unique_slide_ids = df_train_val['slide_submitter_id'].unique()\n",
    "print(len(unique_slide_ids))\n",
    "\n",
    "# Generate indices for cross-validation\n",
    "for train_idx, val_idx in kf.split(unique_slide_ids):\n",
    "    train_slide_ids = unique_slide_ids[train_idx]\n",
    "    val_slide_ids = unique_slide_ids[val_idx]\n",
    "    folds.append((train_slide_ids, val_slide_ids))\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(folds):\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    print(f\"  Training slides: {len(train_ids)}\")\n",
    "    print(f\"  Validation slides: {len(val_ids)}\")\n",
    "print(f\"Train/val patches: {len(df_train_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MILDataset(Dataset):\n",
    "    def __init__(self, bag_data):\n",
    "        self.bag_data = bag_data\n",
    "        self.slide_ids = list(bag_data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bag_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_id = self.slide_ids[idx]\n",
    "        features = torch.tensor(self.bag_data[slide_id]['features'], dtype=torch.float32)\n",
    "        label = torch.tensor(self.bag_data[slide_id]['label'], dtype=torch.float32)\n",
    "        return features, label  # Returns patch features and slide-level continuous label\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    features = [item[0] for item in batch]  # List of feature tensors\n",
    "    labels = torch.tensor([item[1] for item in batch])  # List of labels\n",
    "    return features, labels\n",
    "\n",
    "class PatchClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PatchClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=1536, out_features=256)  # Adjust input size as needed\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc_last = nn.Linear(64, 1)  # Output one probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc_last(x))  # Sigmoid to get probabilities\n",
    "        return x\n",
    "    \n",
    "def pearson_correlation_loss(pred, target):\n",
    "    pred = pred - pred.mean(dim=0)\n",
    "    target = target - target.mean(dim=0)\n",
    "    \n",
    "    numerator = torch.sum(pred * target, dim=0)\n",
    "    denominator = torch.sqrt(torch.sum(pred ** 2, dim=0) * torch.sum(target ** 2, dim=0))\n",
    "    \n",
    "    return -torch.mean(numerator / (denominator + 1e-8))  # Add small epsilon for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features_incremental(df_train, df_val, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Standardizes the features in df_train and df_val using an incremental approach.\n",
    "    This avoids fitting the scaler to the entire dataset at once, making it more memory efficient.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_train: pandas DataFrame containing the training data\n",
    "    - df_val: pandas DataFrame containing the validation data\n",
    "    - feature_columns: list of feature column names (if None, assumes all columns except 'label' and identifiers)\n",
    "    \n",
    "    Returns:\n",
    "    - df_train: pandas DataFrame with standardized features for training\n",
    "    - df_val: pandas DataFrame with standardized features for validation\n",
    "    - scaler: StandardScaler object fitted on the training data\n",
    "    \"\"\"\n",
    "    if feature_columns is None:\n",
    "        feature_columns = df_train.columns[:-2]  # Assuming last two columns are 'label' and identifier\n",
    "    \n",
    "    # Initialize variables for incremental mean and variance computation\n",
    "    n_samples = 0\n",
    "    mean_sum = np.zeros(len(feature_columns))\n",
    "    var_sum = np.zeros(len(feature_columns))\n",
    "    \n",
    "    # Incrementally compute the mean and variance for the training data\n",
    "    for slide_id, group in df_train.groupby('slide_submitter_id'):\n",
    "        features = group[feature_columns].values\n",
    "        n_samples += features.shape[0]\n",
    "        mean_sum += features.sum(axis=0)\n",
    "        var_sum += (features ** 2).sum(axis=0)\n",
    "    \n",
    "    # Final mean and variance\n",
    "    mean = mean_sum / n_samples\n",
    "    variance = (var_sum / n_samples) - (mean ** 2)\n",
    "    std_dev = np.sqrt(np.maximum(variance, 1e-8))  # Avoid division by zero\n",
    "    \n",
    "    # Apply the standardization to both the training and validation data\n",
    "    df_train[feature_columns] = (df_train[feature_columns] - mean) / std_dev\n",
    "    df_val[feature_columns] = (df_val[feature_columns] - mean) / std_dev\n",
    "    \n",
    "    # Create a scaler object that contains the mean and standard deviation\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = mean\n",
    "    scaler.scale_ = std_dev\n",
    "    scaler.var_ = variance\n",
    "    \n",
    "    return df_train, df_val, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1\n",
      "Number of tiles in training set for Fold 1: 174395\n",
      "Number of tiles in validation set for Fold 1: 46153\n",
      "Epoch 1/20, Training Loss: -0.28565680235624313, Validation Loss: -0.610321432352066\n",
      "Epoch 2/20, Training Loss: -0.6551722325384617, Validation Loss: -0.698473185300827\n",
      "Epoch 3/20, Training Loss: -0.7630407586693764, Validation Loss: -0.7922523617744446\n",
      "Epoch 4/20, Training Loss: -0.7933256924152374, Validation Loss: -0.8038998544216156\n",
      "Epoch 5/20, Training Loss: -0.8630600050091743, Validation Loss: -0.8051016330718994\n",
      "Epoch 6/20, Training Loss: -0.8817200288176537, Validation Loss: -0.8198361694812775\n",
      "Epoch 7/20, Training Loss: -0.9083347544074059, Validation Loss: -0.8336503207683563\n",
      "Epoch 8/20, Training Loss: -0.9196669459342957, Validation Loss: -0.836369127035141\n",
      "Epoch 9/20, Training Loss: -0.9377426356077194, Validation Loss: -0.7945689857006073\n",
      "Epoch 10/20, Training Loss: -0.9393857941031456, Validation Loss: -0.8075180053710938\n",
      "Epoch 11/20, Training Loss: -0.9482917785644531, Validation Loss: -0.8023587763309479\n",
      "Epoch 12/20, Training Loss: -0.9484622627496719, Validation Loss: -0.8387545943260193\n",
      "Epoch 13/20, Training Loss: -0.93033317476511, Validation Loss: -0.834270566701889\n",
      "Epoch 14/20, Training Loss: -0.9364198595285416, Validation Loss: -0.8316380679607391\n",
      "Epoch 15/20, Training Loss: -0.9289187490940094, Validation Loss: -0.8391411602497101\n",
      "Epoch 16/20, Training Loss: -0.9376631826162338, Validation Loss: -0.7921142280101776\n",
      "Epoch 17/20, Training Loss: -0.956225298345089, Validation Loss: -0.8175771832466125\n",
      "Epoch 18/20, Training Loss: -0.9653423577547073, Validation Loss: -0.8313727378845215\n",
      "Epoch 19/20, Training Loss: -0.9727042019367218, Validation Loss: -0.8426322638988495\n",
      "Epoch 20/20, Training Loss: -0.9670240581035614, Validation Loss: -0.8063787221908569\n",
      "Training Fold 2\n",
      "Number of tiles in training set for Fold 2: 180350\n",
      "Number of tiles in validation set for Fold 2: 40198\n",
      "Epoch 1/20, Training Loss: -0.3388146609067917, Validation Loss: -0.3408781085163355\n",
      "Epoch 2/20, Training Loss: -0.6034523770213127, Validation Loss: -0.6209881007671356\n",
      "Epoch 3/20, Training Loss: -0.6751631796360016, Validation Loss: -0.7099046409130096\n",
      "Epoch 4/20, Training Loss: -0.8224998936057091, Validation Loss: -0.7536087036132812\n",
      "Epoch 5/20, Training Loss: -0.8380355164408684, Validation Loss: -0.7750686705112457\n",
      "Epoch 6/20, Training Loss: -0.8440697342157364, Validation Loss: -0.8204139769077301\n",
      "Epoch 7/20, Training Loss: -0.87356948107481, Validation Loss: -0.828676164150238\n",
      "Epoch 8/20, Training Loss: -0.8816155344247818, Validation Loss: -0.8280610144138336\n",
      "Epoch 9/20, Training Loss: -0.9106544554233551, Validation Loss: -0.8496817350387573\n",
      "Epoch 10/20, Training Loss: -0.9011394158005714, Validation Loss: -0.8335314393043518\n",
      "Epoch 11/20, Training Loss: -0.9098280593752861, Validation Loss: -0.8317192494869232\n",
      "Epoch 12/20, Training Loss: -0.8976362496614456, Validation Loss: -0.8329710066318512\n",
      "Epoch 13/20, Training Loss: -0.9116785228252411, Validation Loss: -0.8256670236587524\n",
      "Epoch 14/20, Training Loss: -0.935354933142662, Validation Loss: -0.8739510476589203\n",
      "Epoch 15/20, Training Loss: -0.9516618251800537, Validation Loss: -0.8416987955570221\n",
      "Epoch 16/20, Training Loss: -0.9477576985955238, Validation Loss: -0.8572745621204376\n",
      "Epoch 17/20, Training Loss: -0.9512170702219009, Validation Loss: -0.8584287166595459\n",
      "Epoch 18/20, Training Loss: -0.9664284363389015, Validation Loss: -0.8577147126197815\n",
      "Epoch 19/20, Training Loss: -0.96352419257164, Validation Loss: -0.85533407330513\n",
      "Epoch 20/20, Training Loss: -0.9709941446781158, Validation Loss: -0.8615816831588745\n",
      "Training Fold 3\n",
      "Number of tiles in training set for Fold 3: 186321\n",
      "Number of tiles in validation set for Fold 3: 34227\n",
      "Epoch 1/20, Training Loss: -0.34727991186082363, Validation Loss: -0.431990385055542\n",
      "Epoch 2/20, Training Loss: -0.6538782939314842, Validation Loss: -0.521845281124115\n",
      "Epoch 3/20, Training Loss: -0.7158157639205456, Validation Loss: -0.5684521645307541\n",
      "Epoch 4/20, Training Loss: -0.7893791422247887, Validation Loss: -0.6698581576347351\n",
      "Epoch 5/20, Training Loss: -0.826527789235115, Validation Loss: -0.6988606452941895\n",
      "Epoch 6/20, Training Loss: -0.8921594321727753, Validation Loss: -0.6866011917591095\n",
      "Epoch 7/20, Training Loss: -0.8491195142269135, Validation Loss: -0.7032734155654907\n",
      "Epoch 8/20, Training Loss: -0.8657825961709023, Validation Loss: -0.6695773303508759\n",
      "Epoch 9/20, Training Loss: -0.8968394249677658, Validation Loss: -0.7302723526954651\n",
      "Epoch 10/20, Training Loss: -0.9037100225687027, Validation Loss: -0.609657883644104\n",
      "Epoch 11/20, Training Loss: -0.914083369076252, Validation Loss: -0.6431350111961365\n",
      "Epoch 12/20, Training Loss: -0.9343693181872368, Validation Loss: -0.7090425491333008\n",
      "Epoch 13/20, Training Loss: -0.9346003085374832, Validation Loss: -0.6825168430805206\n",
      "Epoch 14/20, Training Loss: -0.9486039876937866, Validation Loss: -0.7189704179763794\n",
      "Epoch 15/20, Training Loss: -0.951040081679821, Validation Loss: -0.731054812669754\n",
      "Epoch 16/20, Training Loss: -0.9421273022890091, Validation Loss: -0.6846590936183929\n",
      "Epoch 17/20, Training Loss: -0.9297005757689476, Validation Loss: -0.7148825526237488\n",
      "Epoch 18/20, Training Loss: -0.9599446877837181, Validation Loss: -0.7096750438213348\n",
      "Epoch 19/20, Training Loss: -0.9611454233527184, Validation Loss: -0.7324147522449493\n",
      "Epoch 20/20, Training Loss: -0.9724086672067642, Validation Loss: -0.6994870007038116\n",
      "Training Fold 4\n",
      "Number of tiles in training set for Fold 4: 171980\n",
      "Number of tiles in validation set for Fold 4: 48568\n",
      "Epoch 1/20, Training Loss: -0.4479175452142954, Validation Loss: -0.5268531292676926\n",
      "Epoch 2/20, Training Loss: -0.7003343515098095, Validation Loss: -0.6001748740673065\n",
      "Epoch 3/20, Training Loss: -0.7522601373493671, Validation Loss: -0.6375649571418762\n",
      "Epoch 4/20, Training Loss: -0.8323830366134644, Validation Loss: -0.5891522169113159\n",
      "Epoch 5/20, Training Loss: -0.8245932534337044, Validation Loss: -0.623941570520401\n",
      "Epoch 6/20, Training Loss: -0.8471620976924896, Validation Loss: -0.6393917202949524\n",
      "Epoch 7/20, Training Loss: -0.8758510202169418, Validation Loss: -0.6589710414409637\n",
      "Epoch 8/20, Training Loss: -0.9109074920415878, Validation Loss: -0.6686273515224457\n",
      "Epoch 9/20, Training Loss: -0.9285724684596062, Validation Loss: -0.6540059149265289\n",
      "Epoch 10/20, Training Loss: -0.9334435760974884, Validation Loss: -0.6607728600502014\n",
      "Epoch 11/20, Training Loss: -0.9473887830972672, Validation Loss: -0.6597976982593536\n",
      "Epoch 12/20, Training Loss: -0.9579926058650017, Validation Loss: -0.6721024215221405\n",
      "Epoch 13/20, Training Loss: -0.9316407144069672, Validation Loss: -0.6819429695606232\n",
      "Epoch 14/20, Training Loss: -0.9472883716225624, Validation Loss: -0.6615620851516724\n",
      "Epoch 15/20, Training Loss: -0.940103605389595, Validation Loss: -0.6317333579063416\n",
      "Epoch 16/20, Training Loss: -0.9507784619927406, Validation Loss: -0.6532092094421387\n",
      "Epoch 17/20, Training Loss: -0.9508738964796066, Validation Loss: -0.6567479074001312\n",
      "Epoch 18/20, Training Loss: -0.9588354974985123, Validation Loss: -0.7153511047363281\n",
      "Epoch 19/20, Training Loss: -0.9666559621691704, Validation Loss: -0.6686806082725525\n",
      "Epoch 20/20, Training Loss: -0.9450288712978363, Validation Loss: -0.6944139003753662\n",
      "Training Fold 5\n",
      "Number of tiles in training set for Fold 5: 169146\n",
      "Number of tiles in validation set for Fold 5: 51402\n",
      "Epoch 1/20, Training Loss: -0.4251876091584563, Validation Loss: -0.4090932607650757\n",
      "Epoch 2/20, Training Loss: -0.6194580867886543, Validation Loss: -0.525511160492897\n",
      "Epoch 3/20, Training Loss: -0.7581437975168228, Validation Loss: -0.61358842253685\n",
      "Epoch 4/20, Training Loss: -0.8127090185880661, Validation Loss: -0.5937339663505554\n",
      "Epoch 5/20, Training Loss: -0.8746822029352188, Validation Loss: -0.6033447086811066\n",
      "Epoch 6/20, Training Loss: -0.8876648917794228, Validation Loss: -0.6220561265945435\n",
      "Epoch 7/20, Training Loss: -0.9076180085539818, Validation Loss: -0.6155422925949097\n",
      "Epoch 8/20, Training Loss: -0.9143832623958588, Validation Loss: -0.6297660171985626\n",
      "Epoch 9/20, Training Loss: -0.8947723656892776, Validation Loss: -0.6217685043811798\n",
      "Epoch 10/20, Training Loss: -0.9342717528343201, Validation Loss: -0.6161391735076904\n",
      "Epoch 11/20, Training Loss: -0.9296968579292297, Validation Loss: -0.6001662313938141\n",
      "Epoch 12/20, Training Loss: -0.9319884777069092, Validation Loss: -0.6119405925273895\n",
      "Epoch 13/20, Training Loss: -0.9477454125881195, Validation Loss: -0.6370457410812378\n",
      "Epoch 14/20, Training Loss: -0.9487091526389122, Validation Loss: -0.6162685453891754\n",
      "Epoch 15/20, Training Loss: -0.9681422933936119, Validation Loss: -0.6635011434555054\n",
      "Epoch 16/20, Training Loss: -0.9647947698831558, Validation Loss: -0.6305260360240936\n",
      "Epoch 17/20, Training Loss: -0.974049486219883, Validation Loss: -0.6515693962574005\n",
      "Epoch 18/20, Training Loss: -0.9705000221729279, Validation Loss: -0.6398838758468628\n",
      "Epoch 19/20, Training Loss: -0.9757460579276085, Validation Loss: -0.6473088264465332\n",
      "Epoch 20/20, Training Loss: -0.9695185199379921, Validation Loss: -0.6293215155601501\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(folds):\n",
    "    print(f\"Training Fold {fold + 1}\")\n",
    "    \n",
    "    model = PatchClassifier()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    features_train_val = df_train_val.copy()\n",
    "    # Create training and validation datasets\n",
    "    df_train = features_train_val[features_train_val['slide_submitter_id'].isin(train_ids)]\n",
    "    df_val = features_train_val[features_train_val['slide_submitter_id'].isin(val_ids)]\n",
    "\n",
    "    # Count the number of tiles (patches) in training and validation sets\n",
    "    train_tiles_count = df_train['tile_ID'].nunique()  # or use .shape[0] for number of rows\n",
    "    val_tiles_count = df_val['tile_ID'].nunique()      # assuming 'tile_ID' is the identifier for each patch\n",
    "    \n",
    "    print(f\"Number of tiles in training set for Fold {fold + 1}: {train_tiles_count}\")\n",
    "    print(f\"Number of tiles in validation set for Fold {fold + 1}: {val_tiles_count}\")\n",
    "    \n",
    "    remove_columns = ['Unnamed: 0', 'tile_ID', 'sample_submitter_id', 'Section', 'Coord_X', 'Coord_Y']\n",
    "    # Clean the datasets\n",
    "    df_train_clean = df_train.drop(columns=[col for col in remove_columns if col in df_train.columns])\n",
    "    df_val_clean = df_val.drop(columns=[col for col in remove_columns if col in df_val.columns])\n",
    "\n",
    "    #df_train_clean, df_val_clean, scaler = standardize_features_incremental(df_train_clean, df_val_clean)\n",
    "\n",
    "    #Standardize features (fit on training data and transform both train and validation data)\n",
    "    feature_columns = df_train_clean.columns[:-2]  # Select all feature columns except 'label' and 'slide_submitter_id'\n",
    "    scaler = StandardScaler()\n",
    "    df_train_clean[feature_columns] = scaler.fit_transform(df_train_clean[feature_columns])\n",
    "    df_val_clean[feature_columns] = scaler.transform(df_val_clean[feature_columns])\n",
    "    \n",
    "    # Bag the data\n",
    "    bag_data_train = {}\n",
    "    for slide_id, group in df_train_clean.groupby('slide_submitter_id'):\n",
    "        features = group.iloc[:, 0:-2].values\n",
    "        label = group['label'].iloc[0]\n",
    "        bag_data_train[slide_id] = {'features': features, 'label': label}\n",
    "\n",
    "    bag_data_val = {}\n",
    "    for slide_id, group in df_val_clean.groupby('slide_submitter_id'):\n",
    "        features = group.iloc[:, 0:-2].values\n",
    "        label = group['label'].iloc[0]\n",
    "        bag_data_val[slide_id] = {'features': features, 'label': label}\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    mil_dataset_train = MILDataset(bag_data_train)\n",
    "    mil_loader_train = DataLoader(mil_dataset_train, batch_size=28, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    mil_dataset_val = MILDataset(bag_data_val)\n",
    "    mil_loader_val = DataLoader(mil_dataset_val, batch_size=28, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    #TRAIN THE MODEL\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_features, batch_labels in mil_loader_train:\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "            batch_predictions = []\n",
    "                \n",
    "            for features in batch_features:  # Iterate through each tensor in the batch\n",
    "                patch_predictions = model(features)  # Get predictions for the patches\n",
    "                aggregated_prediction = patch_predictions.mean()  # Aggregate predictions\n",
    "                batch_predictions.append(aggregated_prediction)\n",
    "\n",
    "            batch_predictions = torch.stack(batch_predictions)\n",
    "\n",
    "            #batch_labels = batch_labels.unsqueeze(1)\n",
    "            loss = criterion(batch_predictions, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validate the model after each epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():  # No gradient calculation needed during validation\n",
    "            for val_features, val_labels in mil_loader_val:\n",
    "                batch_predictions = []\n",
    "\n",
    "                for features in val_features:  # Iterate through each tensor in the batch\n",
    "                    patch_predictions = model(features)  # Get predictions for the patches\n",
    "                    aggregated_prediction = patch_predictions.mean()  # Aggregate predictions\n",
    "                    batch_predictions.append(aggregated_prediction)\n",
    "\n",
    "                batch_predictions = torch.stack(batch_predictions)\n",
    "        \n",
    "                #val_labels = val_labels.unsqueeze(1)\n",
    "                val_loss = criterion(batch_predictions, val_labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(mil_loader_train)\n",
    "        avg_val_loss = total_val_loss / len(mil_loader_val)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
    "        \n",
    "    # Save the model and scaler for each fold into the dictionaries\n",
    "    #models[fold] = deepcopy(model)  # Save the model for the fold\n",
    "    #scalers[fold] = deepcopy(scaler)  # Save the scaler for the fold\n",
    "  \n",
    "    torch.save(model.state_dict(), f\"saved_models/model_fold_{fold}.pth\")\n",
    "    joblib.dump(scaler, f\"saved_scalers/scaler_fold_{fold}.pkl\")    \n",
    "\n",
    "    # Clear fold-specific variables\n",
    "    del model, scaler, df_train, df_val, df_train_clean, df_val_clean\n",
    "    del bag_data_train, bag_data_val, mil_dataset_train, mil_loader_train\n",
    "    del mil_dataset_val, mil_loader_val\n",
    "    del feature_columns, features\n",
    "    del batch_features, batch_predictions, batch_labels\n",
    "    del features_train_val\n",
    "    #del val_features, val_labels\n",
    "\n",
    "    # Release GPU memory and perform garbage collection\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Save the entire dictionary of models and scalers to one file\n",
    "# torch.save(models, \"models_all_folds.pth\")  # Save the models as a single file\n",
    "# joblib.dump(scalers, \"scalers_all_folds.pkl\")  # Save the scalers as a single file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_orion_dir = \"/home/evos/Outputs/CRC/Orion/1_extract_histopathological_features_macenko/features.txt\"\n",
    "# features_orion = pd.read_csv(features_orion_dir, sep=\"\\t\")\n",
    "# features_orion['slide_submitter_id'] = features_orion['tile_ID'].str.rsplit(\"-registered\", n=1).str[0] + \"-registered\"\n",
    "# print(len(features_orion['slide_submitter_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538\n"
     ]
    }
   ],
   "source": [
    "class MILDatasetTestOrion(Dataset):\n",
    "    def __init__(self, bag_data):\n",
    "        self.bag_data = bag_data\n",
    "        self.slide_ids = list(bag_data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bag_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_id = self.slide_ids[idx]\n",
    "        features = torch.tensor(self.bag_data[slide_id]['features'], dtype=torch.float32)\n",
    "        patch_ids = self.bag_data[slide_id]['patch_ids']  # Add patch IDs\n",
    "        return features, patch_ids  # Returns patch features, slide-level label, and patch IDs\n",
    "\n",
    "remove_columns_test = ['Unnamed: 0', 'sample_submitter_id', 'Section', 'Coord_X', 'Coord_Y']\n",
    "df_test_orion = features_orion.drop(columns=[col for col in remove_columns_test if col in features_orion.columns])\n",
    "\n",
    "#del df_test\n",
    "#del features_orion\n",
    "\n",
    "print(len(df_test_orion.columns))\n",
    "\n",
    "bag_data_test_orion = {}\n",
    "for slide_id, group in df_test_orion.groupby('slide_submitter_id'):\n",
    "    features = group.iloc[:, 0:-2].values  # All feature columns\n",
    "    bag_data_test_orion[slide_id] = {'features': features,  'patch_ids': group['tile_ID'].tolist()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47860/1067591511.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"saved_models/model_fold_{fold}.pth\"))\n"
     ]
    }
   ],
   "source": [
    "#models = torch.load(\"models_all_folds.pth\", weights_only=True)\n",
    "# Initialize dictionaries for models and scalers\n",
    "models = {}\n",
    "scalers = {}\n",
    "\n",
    "# Load models and scalers for each fold\n",
    "for fold in range(5):  # Assuming 5 folds\n",
    "    # Load model\n",
    "    model = PatchClassifier()  # Instantiate the model architecture\n",
    "    model.load_state_dict(torch.load(f\"saved_models/model_fold_{fold}.pth\"))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    models[fold] = model  # Add to the dictionary\n",
    "\n",
    "    # Load scaler\n",
    "    scaler = joblib.load(f\"saved_scalers/scaler_fold_{fold}.pkl\")\n",
    "    scalers[fold] = scaler  # Add to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Fold 1\n",
      "Evaluating Fold 2\n",
      "Evaluating Fold 3\n",
      "Evaluating Fold 4\n",
      "Evaluating Fold 5\n",
      "[0.42656191 0.37049146 0.38543637 0.37549092 0.46691704 0.42564714\n",
      " 0.46560416 0.41733762 0.39347794 0.42698573 0.47002504 0.42530107\n",
      " 0.43472832 0.44170366 0.44268641 0.39817792 0.4126657  0.4835652\n",
      " 0.39749862 0.38917665 0.43143335 0.4069359  0.4392131  0.51717474\n",
      " 0.36174337 0.37028938 0.42914325 0.38505267 0.48752766 0.50002725\n",
      " 0.47828993 0.45373147 0.39175201 0.38046278 0.41884526 0.40048413\n",
      " 0.40467044 0.36746585 0.44885207 0.38580743 0.3720481 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize containers for cross-fold results\n",
    "all_final_predictions = []\n",
    "all_true_labels = []\n",
    "all_patch_probabilities = []\n",
    "all_patch_IDs = []\n",
    "\n",
    "for fold in range(5):  # Assuming 5 folds\n",
    "    print(f\"Evaluating Fold {fold + 1}\")\n",
    "    \n",
    "    # Retrieve the model and scaler for the current fold\n",
    "    model = models[fold]\n",
    "    scaler = scalers[fold]\n",
    "    \n",
    "    # Normalize the test data using the current fold's scaler\n",
    "    normalized_bag_data_orion = deepcopy(bag_data_test_orion)\n",
    "    for slide_id in normalized_bag_data_orion:\n",
    "        # Create a DataFrame with the same column names used during training\n",
    "        features_df = pd.DataFrame(\n",
    "            normalized_bag_data_orion[slide_id]['features'], \n",
    "            columns=scaler.feature_names_in_  # Use feature names from the scaler\n",
    "        )\n",
    "        # Normalize using the scaler\n",
    "        normalized_bag_data_orion[slide_id]['features'] = scaler.transform(features_df)\n",
    "    \n",
    "    # Create the test dataset and DataLoader\n",
    "    mil_dataset_orion = MILDatasetTestOrion(normalized_bag_data_orion)\n",
    "    mil_loader_orion = DataLoader(mil_dataset_orion, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Collect predictions for the current fold\n",
    "    fold_patch_probabilities = []\n",
    "    fold_patch_IDs = []\n",
    "    fold_final_predictions = []\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for features, patch_ids in mil_loader_orion:\n",
    "            # Get patch probabilities (since each bag has size 1)\n",
    "            patch_predictions = model(features.squeeze(0))  # Remove extra dimension\n",
    "            \n",
    "            # Collect patch probabilities for the current slide\n",
    "            fold_patch_probabilities.append(patch_predictions.numpy())\n",
    "            fold_patch_IDs.append(patch_ids)\n",
    "            \n",
    "            # Aggregate to get the final prediction (mean of patch probabilities)\n",
    "            final_prediction = patch_predictions.mean().item()\n",
    "            fold_final_predictions.append(final_prediction)\n",
    "    \n",
    "    # Store fold results\n",
    "    all_patch_probabilities.append(fold_patch_probabilities)\n",
    "    all_patch_IDs.append(fold_patch_IDs)\n",
    "    all_final_predictions.append(fold_final_predictions)\n",
    "\n",
    "average_final_predictions = np.mean(all_final_predictions, axis=0)\n",
    "\n",
    "concatenated_patch_probabilities = []\n",
    "for fold in range(5):\n",
    "    concatenated_patches = np.concatenate(all_patch_probabilities[fold], axis=0)\n",
    "    concatenated_patch_probabilities.append(concatenated_patches)\n",
    "average_patch_probabilities = np.mean(concatenated_patch_probabilities, axis=0)\n",
    "\n",
    "concatenated_patch_IDs = np.concatenate(all_patch_IDs[0], axis=0)\n",
    "\n",
    "print(average_final_predictions)\n",
    "sns.boxplot(data=average_patch_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             tile_ID  stromal_cells  \\\n",
      "0  18459_LSP10353_US_SCAN_OR_001__093059-register...       0.235981   \n",
      "1  18459_LSP10353_US_SCAN_OR_001__093059-register...       0.336926   \n",
      "2  18459_LSP10353_US_SCAN_OR_001__093059-register...       0.364961   \n",
      "3  18459_LSP10353_US_SCAN_OR_001__093059-register...       0.662671   \n",
      "4  18459_LSP10353_US_SCAN_OR_001__093059-register...       0.413016   \n",
      "\n",
      "                                           slide_id Coord_X Coord_Y  \n",
      "0  18459_LSP10353_US_SCAN_OR_001__093059-registered   12800    2048  \n",
      "1  18459_LSP10353_US_SCAN_OR_001__093059-registered   20480   24064  \n",
      "2  18459_LSP10353_US_SCAN_OR_001__093059-registered   14336   22016  \n",
      "3  18459_LSP10353_US_SCAN_OR_001__093059-registered   27648    3072  \n",
      "4  18459_LSP10353_US_SCAN_OR_001__093059-registered   13824   18944  \n"
     ]
    }
   ],
   "source": [
    "#cell_type_name = 'tumor_purity' #tumor_purity, endothelial_cells, T_cells\n",
    "\n",
    "dataframes = []\n",
    "# Iterate through each sublist of tuples and corresponding probabilities\n",
    "for patch_ids, probabilities in zip(concatenated_patch_IDs, average_patch_probabilities):\n",
    "    df = pd.DataFrame({\n",
    "        'tile_ID': patch_ids,\n",
    "        cell_type_name: probabilities\n",
    "    })\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one big DataFrame\n",
    "final_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "final_dataframe['tile_ID'] = final_dataframe['tile_ID'].astype(str)\n",
    "\n",
    "#split_columns = final_dataframe['tile_ID'].str.split(\"_\", 2, expand=True)\n",
    "split_columns = final_dataframe['tile_ID'].str.rsplit(\"_\", n=2, expand=True)\n",
    "\n",
    "# Assign the new columns from the split results\n",
    "final_dataframe['slide_id'] = split_columns[0]\n",
    "final_dataframe['Coord_X'] = split_columns[1]\n",
    "final_dataframe['Coord_Y'] = split_columns[2]\n",
    "\n",
    "print(final_dataframe.head())\n",
    "\n",
    "# full_output_dir = \"/home/evos/Outputs/CRC/Orion/MIL_results/MIL_basic_tests/simple_nn_no_norm_MSE\"\n",
    "#final_dataframe.to_csv(f\"{full_output_dir}/{cell_type}_norm_cor.csv\", sep=\"\\t\", index=False)\n",
    "final_dataframe.to_csv(f\"{full_output_dir}/{output_name}.csv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
